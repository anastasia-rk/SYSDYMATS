\documentclass[a4paper,11pt,twoside]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A generic report compiler for publishing rables and plots for a partcular choise of lags in the NARX system
% A path to media corresponding to the setting is defined below, as well as chosen parameter settings
\makeatletter
%\def\input@path{{../Results_set_C_ny_4_nu_4/}}
\def\dataset{C}
\def\ny{4}
\def\nu{4}
\def\order{2}
\makeatother
% Packages
\usepackage[final]{pdfpages}
\usepackage{verbatim}
\usepackage{inputenc}
\usepackage{graphicx} 
\usepackage{amsmath,amssymb,mathrsfs,amsfonts}
%\usepackage{unicode-math}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{calrsfs}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{eucal}    
\usepackage{amssymb}  
\usepackage{pifont}
\usepackage{color} 
\usepackage{cancel}
\usepackage[toc,page]{appendix}
\usepackage{pgfplots}
\pgfplotsset{every axis/.append style={line width=0.5pt},label style={font=\scriptsize},tick label style={font=\scriptsize},x tick label style={/pgf/number format/.cd,fixed,precision=3, set thousands separator={}},z tick label style={/pgf/number format/.cd,fixed,precision=3, set thousands separator={}}}
\usetikzlibrary{shapes,shadows,arrows,backgrounds,patterns,positioning,automata,calc,decorations.markings,decorations.pathreplacing,bayesnet,arrows.meta}
%\usepackage{tikzexternal}
%\usepackage{tikz}
%\usepgfplotslibrary{external} 
%\tikzexternalize
\usepackage{varwidth}
\usepackage{lscape}
\usepackage{array} 
\usepackage[colorlinks=false,pdfborder={0 0 0}]{hyperref}
\usepackage{tabularx}
\usepackage{textcomp}
\usepackage{multicol} 
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[font=small,labelfont=bf]{caption}                                                           
\usepackage{textcase}
\usepackage{bbm} 
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{soul}
%\usepackage[british]{babel}
\usepackage{wrapfig}
%\usepackage{glossaries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\parindent}{2em}
\setlength{\parskip}{0.5em}
\renewcommand{\baselinestretch}{1.2}
\usepackage[left=2cm, right=2cm, top=2.5cm, bottom=3cm, headheight=13.6pt]{geometry}
\allowdisplaybreaks 
% Bibliography
\usepackage[backend=bibtex,style=ieee,sorting=none]{biblatex} 
\bibliography{bibliography}
\renewcommand*{\bibfont}{\scriptsize}
\makeatletter
\def\input@path{{../Results_set_C_ny_0_nu_4/}}
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\newcommand{\ie}{\textit{i.e.} }
\newcommand{\eg}{\textit{e.g.} }
\newcommand\id{\ensuremath{\mathbbm{1}}} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\eye}{\mathbb{I}}
\DeclareMathOperator{\R}{\rm I\!R}
\DeclareMathOperator{\zeros}{\mathbb{O}}
\DeclareMathOperator{\tr}{\textrm{tr}}
\DeclareMathOperator{\vvec}{\textrm{vec}}
\DeclareMathOperator{\ik}{\mathrm{k}}
\DeclareMathOperator{\ip}{\mathrm{p}}
\DeclareMathOperator{\inn}{\mathrm{n}}
\DeclareMathOperator{\im}{\mathrm{m}}
\DeclareMathOperator{\td}{\mathrm{t}}
\DeclareMathOperator{\kd}{\mathrm{k}}
\DeclareMathOperator{\T}{\mathrm{T}}
\DeclareMathOperator{\K}{\mathrm{K}}
\DeclareMathOperator{\rk}{\mathrm{rk}}
\DeclareMathOperator{\vc}{\mathrm{vec}}
\DeclareSymbolFontAlphabet{\mathcal} {symbols}
\DeclareSymbolFont{symbols}{OMS}{cm}{m}{n}
\DeclareMathAlphabet{\mathbfit}{OML}{cmm}{b}{it}
\makeatother
% Number equations
%\numberwithin{equation}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Theorems
\newtheoremstyle{mytheoremstyle} % name
{.5em}                    % Space above
{.8em}                    % Space below
{\itshape}                % Body font
{1em}                           % Indent amount
{\bfseries}                   % Theorem head font
{:}                          % Punctuation after theorem head
{.5em}                       % Space after theorem head
{}  % Theorem head spec (can be left empty, meaning ‘normal’)

\theoremstyle{mytheoremstyle}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{condition}{Condition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{property}{Property}[section]
\newtheorem{corollary}{Corollary}[section]
\renewcommand\qedsymbol{$\blacksquare$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Nomenclature
\usepackage[intoc]{nomencl}
\makenomenclature

%\usepackage[ruled,chapter]{algorithm}
%\usepackage{float}

%\usepackage{algorithmic}
%\algsetup{linenosize=\scriptsize}
%\usepackage{etoolbox}
%\AtBeginEnvironment{algorithmic}{\scriptsize}
%\renewcommand{\thealgorithm}{\thechapter.\arabic{algorithm}} 
%\usepackage{chngcntr}
%\counterwithin{algorithm}{section}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Captions
%\newcommand{\xLanguage}{british}          % <-- Added this command
%
%\usepackage[\xLanguage]{babel}             % <-- Implemented here
%
%\expandafter\addto\csname captions\xLanguage\endcsname{% <-- and here
%	\renewcommand{\tableshortname}{Table}%
%	\renewcommand{\figureshortname}{Figure}%
%}
%
%\captionsetup[table]{format=plain,indention=1.15cm,justification=justified}
%\captionsetup[figure]{format=plain,indention=1.25cm,justification=justified}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[explicit]{titlesec}
\usepackage{titletoc}
%\titleformat{\section}[block]{\normalfont\Large\rm\filright\bfseries}{\thesection}{1em}{#1}
%\titlecontents{chapter}[1.5em]{}{\scshape\contentslabel{2.3em}}{}{\titlerule*[1pc]{}\contentspage}
%\definecolor{gray75}{gray}{0.5}
%\newcommand{\hsp}{\hspace{20pt}}
%\titleformat{\chapter}[hang]{\huge\scshape\filright\bfseries}{\color{gray75}\thechapter}{20pt}{\begin{tabular}[t]{@{\color{gray75}\vrule width 2pt\hsp}p{0.85\textwidth}}\raggedright#1\end{tabular}}
%\titleformat{name=\chapter,numberless}[display]{}{}{0pt}{\normalfont\huge\bfseries #1} % format for numberless chapters
%\titleformat{\subsection}[block]{\normalfont\large\rm\filright\bfseries}{\thesubsection}{1em}{#1}
%\renewcommand{\sectionmark}[1]{\markright{\thesection ~ \ #1}}
%%\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter ~ \ #1}{}} 
%\pagestyle{fancy}
%%\fancyhf{}
%\fancyhead[RO,LE]{\thepage}
%\fancyhead[RE]{\itshape \nouppercase \rightmark}      % chaptertitle left
%\fancyhead[LO]{\itshape \nouppercase \leftmark}       % sectiontitle right
%\renewcommand{\headrulewidth}{0.5pt} 				   % no rule
%\cfoot{}
\interfootnotelinepenalty=10000

\title{Regularised Least Squares estimation of external parameters}
\begin{document}
	\maketitle
\section*{Preliminaries}
\par Recall the notation from the previous reports:
\begin{itemize}
	\item[] $k = 1, \dots, K$ - index of the dataset used for identification.
	\item[] $t = 1, \dots, T$ - discrete time index.
	\item[] $N_s$ - number of significant terms in the model.
	\item[] $i = 1, \dots, N_s$ - term index.
	\item[] $\Xi = \R^2$ - space of external parameters.
	\item[] $\Theta = \R^{N_s}$ - space of internal parameters.
	\item[]	$\theta^{k}_{i}$ - an internal regression parameter corresponding to the $i$-th term in the $k$-th dataset.
	\item[] $\mathbf{\theta}^k = \{\theta^{k}_{i}\}_{i=1}^{N_s}$ - vector of internal parameters.
\end{itemize}
\par The analysis deals with the time-series data from auxetic foam vibration tests introduced in the previous report. A non-linear FIR filter is considered with the input lag of 3. The filter structure is approximated using polynomial regressors of order $\order$. Number the significant terms and corresponding scaling parameters $\theta$ are estimated via EFOR-CMSS algorithm based on 8 datasets out of 10 available. The remaining to datasets (\dataset3 and \dataset8) are used for model validation. 
\par This report deals only with identification of the relationship between the estimated internal parameters $\mathbf{\theta}^k$ and the pre-specified design settings $\xi^k = (L_{cut}, D_{rlx})$. The first section examines the effect on the polynomial structure and sampling from the small subspace of external parameters on the generalisation of the identified model. The remaining sections briefly describe approaches to improving the estimation procedure. The results section only demonstrates numerical results and does not draw any conclusions as this report is intermediate.
\section{Collinearity in the selected model}
\par Recall the assumed polynomial model that links internal parameters to the external ones:
\begin{equation}\label{eq:polyn}
\theta^{k}_{i}(L_{cut},D_{rlx}) = \beta_0 + \beta_1 L_{cut} + \beta_2 D_{rlx} + \beta_3 L_{cut}^{2} + \beta_4 D_{rlx}^{2} + \beta_5 L_{cut} D_{rlx}, \qquad i=1,\dots,N_s,
\end{equation}
where polynomial coefficients $\mathbf{\beta}^i = \left[ \beta_0 \dots \beta_5 \right]$ are unknown. In the original paper \cite{Wei2008} they are estimated via batch LS estimator as follows. First, the matrix of "independent" variables is formed from the external parameter values:
\begin{equation}
\mathbf{A} = \left[\begin{array}{cccccc}
1 & L& D& L\times L& D\times D& L\times D
\end{array}\right],
\end{equation}
where $L = \{L_{cut}^{k}\}^{K}_{k=1}$, $D = \{D_{rlx}^{k}\}^{K}_{k=1}$, the number of columns $M + 1$, where $M$ is the order of polynomial \eqref{eq:polyn}, and where $\times$ denotes the by-element product such that
\begin{equation*}
L\times L = \Big[ L_{cut}^{(k)} L_{cut}^{(k)}\Big]^{K}_{k=1}.
\end{equation*}
Then, for each internal parameter a vector of estimated values is formed across $K$ training datasets:
\begin{equation*}
\bar{\theta}^i = \left\{ \theta^{1}_{i}, \theta^{1}_{i}, \dots, \theta^{K}_{i} \right\}^{\top}.
\end{equation*}
Note that while the internal parameters are considered to be fixed in the estimation, their estimates are obtained from the random data and therefore are random variables 
\begin{equation*}
\bar{\mathbf{\theta}}^i \sim \mathcal{N}(\mu_i, \sigma^{2}_{i}).
\end{equation*}
\begin{remark}
For the purposes of this analysis the factors contributing to this variance are not considered and the vector of LSEs is treated as a response signal in the LS formulation.
\end{remark}
Estimation of the polynomial coefficients for the $i$-th internal parameter $\mathbf{\beta}^i$ is an ordinary least squares (OLS) problem:
\begin{equation}
\hat{\mathbf{\beta}}^i = \underset{\beta \in \R}{\arg \min} \Biggl\{\norm{ \biggl(\bar{\theta}^i - \mathbf{A} \mathbf{\beta}^i \biggr) }^{2}_{2}\Biggr\}
\end{equation}
with closed form solution
\begin{equation}
\hat{\mathbf{\beta}}^{i}_{OLS} = \mathbf{R}^{-1}_{aa} \mathbf{A}^{\top}\bar{\theta}^i,  
\end{equation} 
where $\mathbf{R}^{\ast}_{aa} = (\mathbf{A}^\ast)^{\top}\mathbf{A}^\ast$ is the regression matrix. The OLS estimator is unbiased and provides the solution that best fits the training data, where the fit is characterised by the sum of squared residuals (RSS). The unbiasedness is usually at the cost of the estimate variance and thus does not guarantee good generalisation of the model with estimated parameters. 
\par It can be seen from Table 1 in previous report that the experimental data is collected from the narrow subspace of design parameters. Combined with the polynomial structure that means that the columns of the data matrix $\mathbf{A} $ are powers of one another, this may lead to severe collinearity problem. The quickest way to access collinearity is to check the condition number of the scaled matrix $\mathbf{A}^\ast$. As per recommendation in [cite Seber and Lee], the data is not centred to see the effect on the raw data perturbation on the LSE variance:
\begin{equation*}
a^{\ast}_{k,j} = \frac{a_{k,j}}{\Big\{ \sum^{K}_{k=1} a^{2}_{k,j}\Big\}^{1/2}}, 
\end{equation*}
where $j$ is the column index.
For the given data $\kappa(\mathbf{A}^\ast) = 1083.57$ which indicates that some eigenvalues of the regression matrix $\mathbf{R}^{\ast}_{aa} = (\mathbf{A}^\ast)^{\top}\mathbf{A}^\ast$ are close to zero:
\begin{equation*}
\text{eig}(\mathbf{R}^{\ast}_{aa}) = \left[\begin{array}{r}
4.78 \times 10^{-6} \\
3.50 \times 10^{-5} \\
0.0015	\\
0.0199	\\
0.3625	\\
5.6161	\\
\end{array}\right]
\end{equation*}
Some collinearity metrics for centred and scaled matrix $\mathbf{A}_{s}^{\star}$, where $\mathbf{A} = \left[\right]1, \mathbf{A}_{s}$ are summarised in Table 1. The correlation of each columns and linear combinations of other columns is assessed by computing the coefficients of determination for individual columns
\begin{equation*}
R^{2}_{j} = 1 - \frac{(a^{\star}_{j})^{\top} H_j a^{\star}_{j}}{\sum_{i}( a^{\star}_{j} - \hat{a}^{\star}_{j})^2},
\end{equation*} 
where $ \mathbf{H}_j = \mathbf{A}_{j}^{\star}((\mathbf{A}_{j}^{\star})^{\top}\mathbf{A}_{j}^{\star})^{-1}(\mathbf{A}_{j}^{\star})^{\top}$ is the hat matrix and where $\mathbf{A}_{j}^{\star}$ is the data matrix that excludes the column $a^{\star}_{j}$. The ratio in this case quantifies the distance between the column of interest and its linear  regression on other columns. The smaller this distance is, the higher determination coefficient for the column $a^{\star}_{j}$ will be. However, coefficients of determination do not indicate which particular columns have near-linear dependence. Thus, the effect of collinearity on LSE variance is assessed via variance inflation coefficients
\begin{equation*}
\text{VIF}_j = \frac{\text{Var}(\beta_j)}{\sigma^{2}_{j}} = \frac{\sigma^{2}_{j}}{1 - R^{2}_{j}},
\end{equation*}
while the number of correlated column maybe better assessed from eigenvalues of the scaled and centred regression matrix are
\begin{equation*}
\text{eig}(\mathbf{R}^{\star}_{aa}) = \left[\begin{array}{r}
0.0002\\
0.0004\\
0.0075\\
1.9647\\
3.0271\\
8
\end{array}\right]
\end{equation*}
It can be seen that at least two columns of the normalised data matrix suffer from collinearity, and its effect on variance inflation confirms that the model may suffer bad generalisation outside of the considered subspace of design parameters $\xi$. This indicates the need for regularisation of the estimation procedure in order to reduce the dispersion of the parameter LSEs.
\begin{table}[!h]
	\centering
	\caption{Collinearity metrics of the data.}\label{tab:vifs}
	\begin{tabular}{rrrrrr}
		Column & $L$ & $R$ & $L \times R$ & $L \times L$ & $R \times R$ \\
		\hline
		$R^{2}_{j}$ & 0.9992  &  0.9988  &  0.9997  &  0.9992  &  0.9985 \\
		VIF$_j$ & 1256.04 & 862.29 & 2867.46 & 1212.49 & 665.06 \\
		\hline
	\end{tabular}
\end{table}
\section{Regularised Least Squares}
\par A common treatment for models that suffer from bad generalisation and collinearity is to introduce a constrained LS problem, where the constraint is normally posed on the unknown parameter vector:
\begin{equation}\label{eq:rls_constr}
\hat{\mathbf{\beta}}^i = \underset{\beta \in \R}{\arg \min}\norm{ \biggl(\bar{\theta}^i - \mathbf{A} \mathbf{\beta}^i \biggr) }^{2}_{2}, \quad f_{R}(\mathbf{\beta}^i) < \gamma.
\end{equation}
where $\gamma$ is a pre-specified parameter that defines the size of the constraint in the parameter space $\Xi$.
Lagrangian formulation of the constrained problem is called Regularised Least Squares (RLS),
\begin{equation}\label{eq:rls_problem}
\hat{\mathbf{\beta}}^i = \underset{\beta \in \R}{\arg \min} \Biggl\{\norm{ \biggl(\bar{\theta}^i - \mathbf{A} \mathbf{\beta}^i \biggr) }^{2}_{2} + \lambda f_{R}(\mathbf{\beta}^i)\Biggr\}
\end{equation}
where the regularisation coefficient $\lambda$ is directly linked to  $\gamma$ in \eqref{eq:rls_constr} CITE. Different types of the constraint function can be considered depending on the problem.
\subsection{Tikhonov regularisation}
Innovation regularisation  constrains the 2-norm of the parameter vector
\begin{equation}
f_{R}(\mathbf{\beta}^i) = \norm{\mathbf{\beta}^i}^{2}_{2}.
\end{equation}
This is the only RLS formulation that has a closed form solution that is usually obtained for the normalised data. 
\begin{equation}
\hat{\mathbf{\beta}}^{i}_{RLS} = (\mathbf{R}^{*}_{aa} + \lambda \eye_M)^{-1} (\mathbf{A}^{\star})^{\top}\bar{\theta}^i,  
\end{equation}
This solution is referred to as ridge regression, because increasing $\lambda$ shrinks the coefficients $\beta_j$. The shrinkage is shown is best interpreted via eigenvalue decomposition of the regression matrix.\\
Derivation here\\
This shows that in Tikhonov regularisation $\lambda$ quantifies the trade-off between the bias and the variance in the estimates. Small regularisation coefficient leads to near-OLS solution that overfits the model to the training data, while large value leads to biased estimates and drives all coefficients to near-zero values.
\subsection{LASSO regularisation}
\par While in the ridge regression LSEs asymptotically approach zero, none of the parameters can be zeroed-out explicitly if the model structure is overly detailed. Another formulation of RLS, called Least absolute shrinkage and selection operator (LASSO) regression, performs variable selection and the regularisation simultaneously. The regularising term is the 1-norm of the parameter vector:
\begin{equation}
f_{R}(\mathbf{\beta}^i) = \norm{\mathbf{\beta}^i}_{1}.
\end{equation}
\par zeroing out schemes.
\begin{remark}
	Results of ridge estimation can be used as the initial point in convex optimisation while solving the LASSO regression. 
\end{remark}
\subsection{Selection of the regularisation coefficient}
\par The important stage of solving RLS problem is selecting the regularisation parameter that will result into a interpretable but parsimonious model. The constraint $\gamma$ in \eqref{eq:rls_constr} is often selected arbitrary since the shape of the parameter space is unknown. As a result, finding $\lambda$ relies on iterative schemes most of which do not guarantee convergence [CITE MANY]. For the lack of universal approach for selecting the optimal value of $\lambda$, the choice of the method remains application-specific.
\par In this report, ridge estimation is applied to a fixed model structure, hence Aikaike’s information criterion (AIC) may be used:
\begin{equation}
\text{AIC}_{\lambda} = p + \frac{1}{2}\log p(\bar{\theta}^i \mid \mathbf{\beta}^i, \lambda),
\end{equation}
where the first term quantifies model complexity and the second term is the log-likelihood of the selected model fitting the data. The model complexity is determined as simply the trace of the hat matrix of the ridge estimator
\begin{equation}
p = \tr(\mathbf{H}_RLS) = \tr((\mathbf{A}^{\top})(\mathbf{R}_{aa} + \lambda \eye_M)^{-1} \mathbf{A}^{\top})
\end{equation}.
\section{Results}
\par The results are obtained for the auxetic form data for 8 datasets using both types of regularisation. Ridge traces are shown for values of $\lambda$ from $10^{-6}$ to $10^0$. While the RLS is usually used for the normalised (scaled and centred) data, results for the raw   

\section{Future work}
\begin{itemize}
	\item Regularised LS for the direct estimation of $B$ from joint datasets. Long time-series and polynomial structure of the regressors may also lead to collinearity problems. It's not clear yet whether the data needs to be normalised separately.
	\item Confidence and covariance regions to demonstrate bias-variance trade off.
	\item Multiple methods for LASSO regression can be investigated.
	\item Cross-validation and other methods to select the regularisation term.
\end{itemize}
\end{document}